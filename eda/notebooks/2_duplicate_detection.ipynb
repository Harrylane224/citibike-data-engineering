{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a256ceac",
   "metadata": {},
   "source": [
    "# **Citi Bike Data Engineering  - EDA - Duplicate Detection** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97baae9",
   "metadata": {},
   "source": [
    "#### Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa81a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from itertools import combinations\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e922196",
   "metadata": {},
   "source": [
    "#### Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2ef689b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from scripts.create_markdown_table import create_markdown_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d393304",
   "metadata": {},
   "source": [
    "#### Import DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "362b0781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "df_dir = os.path.join(project_root, 'eda', 'dataframes')\n",
    "newark_airport_df = pd.read_pickle(os.path.join(df_dir, 'newark_airport_df.pkl'))\n",
    "citibike_df = pd.read_pickle(os.path.join(df_dir, 'citibike_df.pkl'))\n",
    "\n",
    "# Confirm that the dataframes are loaded correctly\n",
    "if newark_airport_df.empty or citibike_df.empty:\n",
    "    raise ValueError(\"One or both dataframes are empty. Please check the data loading process.\")\n",
    "else:\n",
    "    print(\"Dataframes loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae8bf30",
   "metadata": {},
   "source": [
    "#### Assess whether duplicate rows exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cb74943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows in newark_airport_df: 0\n",
      "Number of duplicate rows in citibike_df: 0\n"
     ]
    }
   ],
   "source": [
    "# Return the number of duplicate rows in newark_airport_df\n",
    "newark_airport_duplicates = newark_airport_df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in newark_airport_df: {newark_airport_duplicates}\")\n",
    "# Return the number of duplicate rows in citibike_df\n",
    "citibike_duplicates = citibike_df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in citibike_df: {citibike_duplicates}\")\n",
    "\n",
    "# Create a markdown table to return the results of the duplicate counts\n",
    "duplicate_counts = {\n",
    "    'DataFrame': ['newark_airport_df', 'citibike_df'],\n",
    "    'Duplicate Rows': [newark_airport_duplicates, citibike_duplicates]\n",
    "}\n",
    "duplicate_counts_df = pd.DataFrame(duplicate_counts)\n",
    "markdown_table = create_markdown_table(duplicate_counts_df, 'duplicate_counts_table')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa2f37c",
   "metadata": {},
   "source": [
    "#### Determine uniqueness of identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18ab93e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential primary keys in newark_airport_df: ['DATE']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential primary keys in test_df: ['STATION', 'NAME', 'AWND', 'PGTM', 'PRCP', 'SNOW', 'SNWD', 'TAVG', 'TMAX', 'TMIN']\n"
     ]
    }
   ],
   "source": [
    "# Create a global dataframe to store the results from the newark_airport to be used in the markdown table\n",
    "pk_results_newark_df = pd.DataFrame(columns=['Dataframe', 'Potential Primary Key(s)'])\n",
    "\n",
    "# Write a function that identifies potential primary keys in a dataframe\n",
    "def find_primary_keys(df):\n",
    "    num_rows = len(df)\n",
    "    columns = df.columns.tolist()\n",
    "\n",
    "    # Check 1: Check individual columns\n",
    "    for col in columns:\n",
    "        if df[col].is_unique and df[col].notna().all():\n",
    "            return [col]\n",
    "\n",
    "    # Check 2: Attempting combining columns to identify a primary key\n",
    "    for r in range(2, len(columns) + 1):\n",
    "        cols = columns[:r]\n",
    "        combined = df[cols].astype(str).agg('-'.join, axis=1)\n",
    "        if combined.is_unique:\n",
    "            return cols\n",
    "\n",
    "    # Check 3: If there are no unique combinations, return nothing\n",
    "    return None\n",
    "\n",
    "# Find potential primary keys in newark_airport_df\n",
    "newark_airport_primary_keys = find_primary_keys(newark_airport_df)\n",
    "if newark_airport_primary_keys:\n",
    "    print(f\"Potential primary keys in newark_airport_df: {newark_airport_primary_keys}\")\n",
    "else:\n",
    "    print(\"No potential primary keys found in newark_airport_df.\")\n",
    "\n",
    "# Add the results to the global dataframe\n",
    "pk_results_newark_df = pd.concat([pk_results_newark_df, pd.DataFrame({\n",
    "    'Dataframe': ['newark_airport_df'],\n",
    "    'Potential Primary Key(s)': [', '.join(newark_airport_primary_keys)]\n",
    "})], ignore_index=True)\n",
    "\n",
    "# Since column 'DATE' is identified as a potential primary key, we will drop it for the next check\n",
    "sub_newark_airport_df = newark_airport_df.drop(columns=['DATE'])\n",
    "test_df_primary_keys = find_primary_keys(sub_newark_airport_df)\n",
    "if test_df_primary_keys:\n",
    "    print(f\"Potential primary keys in test_df: {test_df_primary_keys}\")\n",
    "else:\n",
    "    print(\"No potential primary keys found in test_df.\")\n",
    "\n",
    "# Add the results to the global dataframe\n",
    "pk_results_newark_df = pd.concat([\n",
    "    pk_results_newark_df,\n",
    "    pd.DataFrame([{\n",
    "        'Dataframe': 'newark_airport_df',\n",
    "        'Potential Primary Key(s)': ', '.join(test_df_primary_keys)\n",
    "    }])\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bf7c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential primary keys in citibike_df: ['index']\n",
      "Potential primary keys in test_df: ['Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID', 'Start Station Name', 'Start Station Latitude', 'Start Station Longitude', 'End Station ID', 'End Station Name', 'End Station Latitude', 'End Station Longitude', 'Bike ID']\n"
     ]
    }
   ],
   "source": [
    "# Create a global dataframe to store the results from the citibike to be used in the markdown table\n",
    "pk_results_citibike_df = pd.DataFrame(columns=['Dataframe', 'Potential Primary Key(s)'])\n",
    "\n",
    "# Find potential primary keys in citibike_df\n",
    "citibike_primary_keys = find_primary_keys(citibike_df)\n",
    "if citibike_primary_keys:\n",
    "    print(f\"Potential primary keys in citibike_df: {citibike_primary_keys}\")\n",
    "else:\n",
    "    print(\"No potential primary keys found in citibike_df.\")\n",
    "\n",
    "# Add the results to the global dataframe\n",
    "pk_results_citibike_df = pd.concat([pk_results_citibike_df, pd.DataFrame({\n",
    "    'Dataframe': ['citibike_df'],\n",
    "    'Potential Primary Key(s)': [', '.join(citibike_primary_keys)]\n",
    "})], ignore_index=True)\n",
    "\n",
    "# Since column 'index' is identified as a potential primary key, we will drop it for the next check\n",
    "sub_citibike_df = citibike_df.drop(columns=['index'])\n",
    "test_df_primary_keys = find_primary_keys(sub_citibike_df)\n",
    "if test_df_primary_keys:\n",
    "    print(f\"Potential primary keys in test_df: {test_df_primary_keys}\")\n",
    "else:\n",
    "    print(\"No potential primary keys found in test_df.\")\n",
    "\n",
    "# Add the results to the global dataframe\n",
    "pk_results_citibike_df = pd.concat([\n",
    "    pk_results_citibike_df,\n",
    "    pd.DataFrame([{\n",
    "        'Dataframe': 'citibike_df',\n",
    "        'Potential Primary Key(s)': ', '.join(test_df_primary_keys)\n",
    "    }])\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ab833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the results from both dataframes into a single dataframe\n",
    "pk_results_df = pd.concat([pk_results_newark_df, pk_results_citibike_df], ignore_index=True)\n",
    "\n",
    "# Create a markdown table from the results\n",
    "markdown_table = create_markdown_table(pk_results_df, 'potential_primary_keys')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
