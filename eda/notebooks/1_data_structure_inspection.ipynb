{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c55ac1",
   "metadata": {},
   "source": [
    "# **Citi Bike Data Engineering  - EDA - Inspecting Data Structures** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7e09ed",
   "metadata": {},
   "source": [
    "#### Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "726700dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from difflib import get_close_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132e2ee",
   "metadata": {},
   "source": [
    "#### Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c272dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from scripts.create_markdown_table import create_markdown_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d065160",
   "metadata": {},
   "source": [
    "#### Load CSV Files into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f1c44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to the CSV files\n",
    "dir_path = '../../data-sources/data/'\n",
    "\n",
    "# Create a function to read CSV files and return a DataFrame\n",
    "def read_csv_file(file_name):\n",
    "    file_path = dir_path + file_name\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found in directory {dir_path}.\")\n",
    "        return None\n",
    "\n",
    "# Assign the CSV files to variables\n",
    "newark_airport_df = read_csv_file('newark_airport_2016.csv')\n",
    "citibike_201601 = read_csv_file('JC-201601-citibike-tripdata.csv')\n",
    "citibike_201602 = read_csv_file('JC-201602-citibike-tripdata.csv')\n",
    "citibike_201603 = read_csv_file('JC-201603-citibike-tripdata.csv')\n",
    "citibike_201604 = read_csv_file('JC-201604-citibike-tripdata.csv')\n",
    "citibike_201605 = read_csv_file('JC-201605-citibike-tripdata.csv')\n",
    "citibike_201606 = read_csv_file('JC-201606-citibike-tripdata.csv')\n",
    "citibike_201607 = read_csv_file('JC-201607-citibike-tripdata.csv')\n",
    "citibike_201608 = read_csv_file('JC-201608-citibike-tripdata.csv')\n",
    "citibike_201609 = read_csv_file('JC-201609-citibike-tripdata.csv')\n",
    "citibike_201610 = read_csv_file('JC-201610-citibike-tripdata.csv')\n",
    "citibike_201611 = read_csv_file('JC-201611-citibike-tripdata.csv')\n",
    "citibike_201612 = read_csv_file('JC-201612-citibike-tripdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7fc3f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of newark_airport_df:\n",
      "                                                    NAME        DATE   AWND  \\\n",
      "STATION                                                                       \n",
      "USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-01  12.75   \n",
      "USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-02   9.40   \n",
      "USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-03  10.29   \n",
      "USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-04  17.22   \n",
      "USW00014734  NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US  2016-01-05   9.84   \n",
      "\n",
      "             PGTM  PRCP  SNOW  SNWD  TAVG  TMAX  TMIN  TSUN  WDF2   WDF5  \\\n",
      "STATION                                                                    \n",
      "USW00014734   NaN   0.0   0.0   0.0    41    43    34   NaN   270  280.0   \n",
      "USW00014734   NaN   0.0   0.0   0.0    36    42    30   NaN   260  260.0   \n",
      "USW00014734   NaN   0.0   0.0   0.0    37    47    28   NaN   270  250.0   \n",
      "USW00014734   NaN   0.0   0.0   0.0    32    35    14   NaN   330  330.0   \n",
      "USW00014734   NaN   0.0   0.0   0.0    19    31    10   NaN   360  350.0   \n",
      "\n",
      "             WSF2  WSF5  \n",
      "STATION                  \n",
      "USW00014734  25.9  35.1  \n",
      "USW00014734  21.0  25.1  \n",
      "USW00014734  23.9  30.0  \n",
      "USW00014734  25.9  33.1  \n",
      "USW00014734  25.1  31.1  \n",
      "Shape of newark_airport_df: (366, 15)\n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows of the newark_airport DataFrame\n",
    "print(\"First few rows of newark_airport_df:\")\n",
    "print(newark_airport_df.head())\n",
    "\n",
    "# Assess the shape of new_airport DataFrame\n",
    "print(f\"Shape of newark_airport_df: {newark_airport_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c51cfd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of citibike_201601: (7479, 14)\n",
      "Shape of citibike_201602: (8250, 14)\n",
      "Shape of citibike_201603: (13511, 14)\n",
      "Shape of citibike_201604: (16342, 14)\n",
      "Shape of citibike_201605: (19488, 14)\n",
      "Shape of citibike_201606: (23947, 14)\n",
      "Shape of citibike_201607: (24436, 14)\n",
      "Shape of citibike_201608: (34149, 14)\n",
      "Shape of citibike_201609: (33425, 14)\n",
      "Shape of citibike_2016010: (29611, 14)\n",
      "Shape of citibike_2016011: (21832, 14)\n",
      "Shape of citibike_2016012: (15114, 14)\n",
      "Total rows across all citibike DataFrames: 247584\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Due to the large size of each CSV file under the 'citibike...csv' naming convention, this following code\n",
    "will evaluate the shape of each DataFrame to assess whether them would present any performance or structural issues.\n",
    "'''\n",
    "# List all the citibike DataFrames for easier management\n",
    "citibike_dfs = [\n",
    "    citibike_201601, citibike_201602, citibike_201603, citibike_201604,\n",
    "    citibike_201605, citibike_201606, citibike_201607, citibike_201608,\n",
    "    citibike_201609, citibike_201610, citibike_201611, citibike_201612\n",
    "]\n",
    "# Print and assess the number of rows\n",
    "row_count = 0\n",
    "for i, df in enumerate(citibike_dfs, start=1):\n",
    "    if df is not None:\n",
    "        row_count += df.shape[0]\n",
    "        print(f\"Shape of citibike_20160{i}: {df.shape}\")\n",
    "    else:\n",
    "        print(f\"citibike_20160{i} DataFrame is None.\")\n",
    "print(f\"Total rows across all citibike DataFrames: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ea6cd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All DataFrames have a consistent column structure and data types.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assumption: Each citibike DataFrame has a consistent column structure and can be stacked without issues.\n",
    "# Create a function that confirms whether the Assumption is valid\n",
    "def check_column_structure_consistency(dfs):\n",
    "    # If the list of DataFrames is empty, return a message\n",
    "    if not dfs:\n",
    "        return \"No DataFrames provided.\"\n",
    "    # Create a list of column names and data types for the first DataFrame\n",
    "    init_columns = dfs[0].columns.tolist()\n",
    "    init_dtypes = dfs[0].dtypes.tolist()\n",
    "    # If init_columns and init_ are None, return an message\n",
    "    if init_columns is None or init_dtypes is None:\n",
    "        return \"Initial DataFrame has no columns or data types.\"\n",
    "    # Write a for loop that compares init_columns and init_dtypes with the rest of the DataFrames\n",
    "    for i, df in enumerate(dfs, start=1):\n",
    "        count = 0\n",
    "        comp_columns = df.columns.tolist()\n",
    "        comp_dtypes = df.dtypes.tolist()\n",
    "        if comp_columns != init_columns or comp_dtypes != init_dtypes:\n",
    "            return f\"DataFrame citibike_20160{i} has a different structure compared to DataFrame citibike_201601.\"\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        return \"All DataFrames have a consistent column structure and data types.\"\n",
    "\n",
    "\n",
    "check_column_structure_consistency(citibike_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4447d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of concatenated citibike DataFrame: (247584, 16)\n",
      "There is a mismatch in the number of rows or columns after concatenation.\n",
      "First few rows of concatenated citibike_df:\n",
      "   index  Trip Duration           Start Time            Stop Time  \\\n",
      "0      0            362  2016-01-01 00:02:52  2016-01-01 00:08:54   \n",
      "1      1            200  2016-01-01 00:18:22  2016-01-01 00:21:42   \n",
      "2      2            202  2016-01-01 00:18:25  2016-01-01 00:21:47   \n",
      "3      3            248  2016-01-01 00:23:13  2016-01-01 00:27:21   \n",
      "4      4            903  2016-01-01 01:03:20  2016-01-01 01:18:24   \n",
      "\n",
      "   Start Station ID Start Station Name  Start Station Latitude  \\\n",
      "0              3186      Grove St PATH               40.719586   \n",
      "1              3186      Grove St PATH               40.719586   \n",
      "2              3186      Grove St PATH               40.719586   \n",
      "3              3209       Brunswick St               40.724176   \n",
      "4              3195            Sip Ave               40.730743   \n",
      "\n",
      "   Start Station Longitude  End Station ID End Station Name  \\\n",
      "0               -74.043117            3209     Brunswick St   \n",
      "1               -74.043117            3213   Van Vorst Park   \n",
      "2               -74.043117            3213   Van Vorst Park   \n",
      "3               -74.050656            3203    Hamilton Park   \n",
      "4               -74.063784            3210   Pershing Field   \n",
      "\n",
      "   End Station Latitude  End Station Longitude  Bike ID   User Type  \\\n",
      "0             40.724176             -74.050656    24647  Subscriber   \n",
      "1             40.718489             -74.047727    24605  Subscriber   \n",
      "2             40.718489             -74.047727    24689  Subscriber   \n",
      "3             40.727596             -74.044247    24693  Subscriber   \n",
      "4             40.742677             -74.051789    24573    Customer   \n",
      "\n",
      "   Birth Year  Gender  \n",
      "0      1964.0       2  \n",
      "1      1962.0       1  \n",
      "2      1962.0       2  \n",
      "3      1984.0       1  \n",
      "4         NaN       0  \n"
     ]
    }
   ],
   "source": [
    "# Now that we have confirmed the structure consistency, we can concatenate the DataFrames\n",
    "cleaned_dfs = []\n",
    "for df in citibike_dfs:\n",
    "    # Reset existing index to avoid 'level_0' or other named index issues\n",
    "    df = df.reset_index(drop=False)\n",
    "\n",
    "    # Ensure 'Trip Duration' is not being used as the index\n",
    "    if 'Trip Duration' not in df.columns:\n",
    "        raise ValueError(\"'Trip Duration' column not found after reset. Check source data.\")\n",
    "\n",
    "    cleaned_dfs.append(df)\n",
    "\n",
    "# Concatenate all the citibike DataFrames into a single DataFrame\n",
    "citibike_df = pd.concat(cleaned_dfs, ignore_index=True)\n",
    "\n",
    "# Assign a new index to the concatenated DataFrame\n",
    "citibike_df.insert(0, \"index\", range(len(citibike_df)))\n",
    "\n",
    "# Print the shape of the concatenated DataFrame\n",
    "print(f\"Shape of concatenated citibike DataFrame: {citibike_df.shape}\")\n",
    "\n",
    "# Check the shape of the concatenated DataFrame matches the expected structure\n",
    "# Minus one column for the original index\n",
    "if citibike_df.shape[0] == row_count and citibike_df.shape[1] - 1 == len(citibike_201601.columns):\n",
    "    print(\"All DataFrames have been concatenated successfully.\")\n",
    "else:\n",
    "    print(\"There is a mismatch in the number of rows or columns after concatenation.\")\n",
    "\n",
    "# Print the first few rows of the concatenated DataFrame\n",
    "print(\"First few rows of concatenated citibike_df:\")\n",
    "print(citibike_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6c82a",
   "metadata": {},
   "source": [
    "#### Evaluate consistency with provided data dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1d1a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the column descriptions for the Newark Airport dataset, referring to weather.pdf\n",
    "# Assumption: All values are in American units, no description that confirms within data dictionary\n",
    "newark_column_descriptions = {\n",
    "    'STATION': ' The station identification code',\n",
    "    'NAME': 'Name of the station (Newark Airport)',\n",
    "    'DATE': 'Date of the observation (YYYY-MM-DD)',\n",
    "    'AWND': 'Average wind speed (miles per hour)',\n",
    "    'PGTM': 'Peak gust time (HHMM)',\n",
    "    'PRCP': 'Precipitation (inches)',\n",
    "    'SNOW': 'Snowfall (inches)',\n",
    "    'SNWD': 'Snow depth (inches)',\n",
    "    'TAVG': 'Average temperature (degrees Fahrenheit)',\n",
    "    'TMAX': 'Maximum temperature (degrees Fahrenheit)',\n",
    "    'TMIN': 'Minimum temperature (degrees Fahrenheit)',\n",
    "    'TSUN': 'Total sunshine (minutes)',\n",
    "    'WDF2': 'Direction of fastest 2-minute wind (degrees)',\n",
    "    'WDF5': 'Direction of fastest 5-second wind (degrees)',\n",
    "    'WSF2': 'Fastest 2-minute wind speed (miles per hour)',\n",
    "    'WSF5': 'Fastest 5-second wind speed (miles per hour)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dae402f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of the column descriptions for the Newark Airport dataset, referring to weather.pdf\n",
    "# Assumption: All values are in American units, no description that confirms within data dictionary\n",
    "citibike_column_descriptions = {\n",
    "    'index': 'Original index of the DataFrame',\n",
    "    'Trip Duration': 'Duration of the trip in seconds',\n",
    "    'Start Time': 'Start time of the trip (YYYY-MM-DD HH:MM:SS)',\n",
    "    'Stop Time': 'End time of the trip (YYYY-MM-DD HH:MM:SS)',\n",
    "    'Start Station ID': 'Unique identifier for the start station',\n",
    "    'Start Station Name': 'Name of the start station',\n",
    "    'Start Station Latitude': 'Latitude of the start station',\n",
    "    'Start Station Longitude': 'Longitude of the start station',\n",
    "    'End Station ID': 'Unique identifier for the end station',\n",
    "    'End Station Name': 'Name of the end station',\n",
    "    'End Station Latitude': 'Latitude of the end station',\n",
    "    'End Station Longitude': 'Longitude of the end station',\n",
    "    'Bike ID': 'Unique identifier for the bike used in the trip',\n",
    "    'User Type': 'Type of user (Customer = 24-hour pass or 3-day pass user; Subscriber = Annual Member)',\n",
    "    'Birth Year': 'Year of birth of the user',\n",
    "    'Gender': 'Gender of the user (0: Unknown, 1: Male, 2: Female)'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a5e7d",
   "metadata": {},
   "source": [
    "#### Create lists of column names, data types and inferred domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3689981",
   "metadata": {},
   "source": [
    "##### Newark Airport DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25a74891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newark Airport DataFrame Information:\n",
      "   Column Name                                   Description Data Type  \\\n",
      "0      STATION               The station identification code    object   \n",
      "1         NAME          Name of the station (Newark Airport)    object   \n",
      "2         DATE          Date of the observation (YYYY-MM-DD)    object   \n",
      "3         AWND           Average wind speed (miles per hour)   float64   \n",
      "4         PGTM                         Peak gust time (HHMM)   float64   \n",
      "5         PRCP                        Precipitation (inches)   float64   \n",
      "6         SNOW                             Snowfall (inches)   float64   \n",
      "7         SNWD                           Snow depth (inches)   float64   \n",
      "8         TAVG      Average temperature (degrees Fahrenheit)     int64   \n",
      "9         TMAX      Maximum temperature (degrees Fahrenheit)     int64   \n",
      "10        TMIN      Minimum temperature (degrees Fahrenheit)     int64   \n",
      "11        TSUN                      Total sunshine (minutes)   float64   \n",
      "12        WDF2  Direction of fastest 2-minute wind (degrees)     int64   \n",
      "13        WDF5  Direction of fastest 5-second wind (degrees)   float64   \n",
      "14        WSF2  Fastest 2-minute wind speed (miles per hour)   float64   \n",
      "15        WSF5  Fastest 5-second wind speed (miles per hour)   float64   \n",
      "\n",
      "                                   Value Examples  Value Distinct Count  \n",
      "0                                   [USW00014734]                     1  \n",
      "1   [NEWARK LIBERTY INTERNATIONAL AIRPORT, NJ US]                     1  \n",
      "2            [2016-01-01, 2016-01-02, 2016-01-03]                   366  \n",
      "3                             [12.75, 9.4, 10.29]                    70  \n",
      "4                                              []                     0  \n",
      "5                               [0.0, 0.01, 1.77]                    56  \n",
      "6                                 [0.0, 0.7, 0.5]                    11  \n",
      "7                                 [0.0, 1.2, 7.1]                    12  \n",
      "8                                    [41, 36, 37]                    69  \n",
      "9                                    [43, 42, 47]                    74  \n",
      "10                                   [34, 30, 28]                    69  \n",
      "11                                             []                     0  \n",
      "12                                [270, 260, 330]                    34  \n",
      "13                          [280.0, 260.0, 250.0]                    35  \n",
      "14                             [25.9, 21.0, 23.9]                    32  \n",
      "15                             [35.1, 25.1, 30.0]                    44  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create a Dataframe that provides the following information for newark_airport_df:\n",
    "    - Column Name\n",
    "    - Data Type\n",
    "    - Column Description\n",
    "    - Value Examples\n",
    "    - Value Distinct Count\n",
    "'''\n",
    "\n",
    "# Create a summary DataFrame for the Newark Airport dataset\n",
    "# Add the column descriptions to the summary DataFrame\n",
    "summary = []\n",
    "newark_airport_df = newark_airport_df.reset_index()\n",
    "for col in newark_airport_df.columns:\n",
    "    summary.append({\n",
    "        'Column Name': col,\n",
    "        'Description': newark_column_descriptions.get(col, ''),\n",
    "        'Data Type': newark_airport_df[col].dtype,\n",
    "        'Value Examples': newark_airport_df[col].dropna().unique()[:3],\n",
    "        'Value Distinct Count': newark_airport_df[col].nunique()\n",
    "    })\n",
    "newark_airport_shape = pd.DataFrame(summary)\n",
    "\n",
    "# Print the newark_airport_shape DataFrame\n",
    "print(\"Newark Airport DataFrame Information:\")\n",
    "print(newark_airport_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2008d36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Markdown table 'newark_airport_shape' created successfully in 'outputs/'.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the newark_airport_shape DataFrame as a markdown table for the eda_insights.md file\n",
    "create_markdown_table(newark_airport_shape, \"newark_airport_shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821b308f",
   "metadata": {},
   "source": [
    "##### Citi Bike DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0707a7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citi Bike DataFrame Information:\n",
      "                Column Name  \\\n",
      "0                     index   \n",
      "1             Trip Duration   \n",
      "2                Start Time   \n",
      "3                 Stop Time   \n",
      "4          Start Station ID   \n",
      "5        Start Station Name   \n",
      "6    Start Station Latitude   \n",
      "7   Start Station Longitude   \n",
      "8            End Station ID   \n",
      "9          End Station Name   \n",
      "10     End Station Latitude   \n",
      "11    End Station Longitude   \n",
      "12                  Bike ID   \n",
      "13                User Type   \n",
      "14               Birth Year   \n",
      "15                   Gender   \n",
      "\n",
      "                                          Description Data Type  \\\n",
      "0                     Original index of the DataFrame     int64   \n",
      "1                     Duration of the trip in seconds     int64   \n",
      "2        Start time of the trip (YYYY-MM-DD HH:MM:SS)    object   \n",
      "3          End time of the trip (YYYY-MM-DD HH:MM:SS)    object   \n",
      "4             Unique identifier for the start station     int64   \n",
      "5                           Name of the start station    object   \n",
      "6                       Latitude of the start station   float64   \n",
      "7                      Longitude of the start station   float64   \n",
      "8               Unique identifier for the end station     int64   \n",
      "9                             Name of the end station    object   \n",
      "10                        Latitude of the end station   float64   \n",
      "11                       Longitude of the end station   float64   \n",
      "12    Unique identifier for the bike used in the trip     int64   \n",
      "13  Type of user (Customer = 24-hour pass or 3-day...    object   \n",
      "14                          Year of birth of the user   float64   \n",
      "15  Gender of the user (0: Unknown, 1: Male, 2: Fe...     int64   \n",
      "\n",
      "                                       Value Examples  Value Distinct Count  \n",
      "0                                           [0, 1, 2]                247584  \n",
      "1                                     [362, 200, 202]                  6024  \n",
      "2   [2016-01-01 00:02:52, 2016-01-01 00:18:22, 201...                244407  \n",
      "3   [2016-01-01 00:08:54, 2016-01-01 00:21:42, 201...                244137  \n",
      "4                                  [3186, 3209, 3195]                    51  \n",
      "5              [Grove St PATH, Brunswick St, Sip Ave]                    51  \n",
      "6   [40.71958611647166, 40.7241765, 40.73074262530...                    51  \n",
      "7   [-74.04311746358871, -74.0506564, -74.06378388...                    51  \n",
      "8                                  [3209, 3213, 3203]                   102  \n",
      "9       [Brunswick St, Van Vorst Park, Hamilton Park]                   102  \n",
      "10            [40.7241765, 40.71848892, 40.727595966]                   102  \n",
      "11        [-74.0506564, -74.047726625, -74.044247311]                   102  \n",
      "12                              [24647, 24605, 24689]                   566  \n",
      "13                             [Subscriber, Customer]                     2  \n",
      "14                           [1964.0, 1962.0, 1984.0]                    64  \n",
      "15                                          [2, 1, 0]                     3  \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create a Dataframe that provides the following information for citibike_df:\n",
    "    - Column Name\n",
    "    - Data Type\n",
    "    - Column Description\n",
    "    - Value Examples\n",
    "    - Value Distinct Count\n",
    "'''\n",
    "\n",
    "# Create a summary DataFrame for the Citi Bike DataFrame\n",
    "# Add the column descriptions to the summary DataFrame\n",
    "summary = []\n",
    "for col in citibike_df.columns:\n",
    "    summary.append({\n",
    "        'Column Name': col,\n",
    "        'Description': citibike_column_descriptions.get(col, ''),\n",
    "        'Data Type': citibike_df[col].dtype,\n",
    "        'Value Examples': citibike_df[col].dropna().unique()[:3],\n",
    "        'Value Distinct Count': citibike_df[col].nunique()\n",
    "    })\n",
    "citibike_shape = pd.DataFrame(summary)\n",
    "\n",
    "# Print the citibike_shape DataFrame\n",
    "print(\"Citi Bike DataFrame Information:\")\n",
    "print(citibike_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51a9d1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Markdown table 'citibike_shape' created successfully in 'outputs/'.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the citibike_shape DataFrame as a markdown table for the eda_insights.md file\n",
    "create_markdown_table(citibike_shape, \"citibike_shape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b50845",
   "metadata": {},
   "source": [
    "#### Detect column groupings for entity relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc7d2b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No similar column names found.\n"
     ]
    }
   ],
   "source": [
    "# Create function that uses get_closest_match to identify columns with similar matches between newark_airport_df and citibike_df\n",
    "def find_similar_column_names(df1, df2, cutoff=0.75):\n",
    "    similar_cols = []\n",
    "    for col1 in df1.columns:\n",
    "        matches = get_close_matches(col1, df2.columns, n=3, cutoff=cutoff)\n",
    "        for match in matches:\n",
    "            similar_cols.append({\n",
    "                \"df1_column\": col1,\n",
    "                \"df2_column\": match,\n",
    "                \"note\": \"similar name\"\n",
    "            })\n",
    "    if not similar_cols:\n",
    "        print(\"No similar column names found.\")\n",
    "    else:\n",
    "        print(\"Similar column names found:\")\n",
    "        print(similar_cols)\n",
    "\n",
    "find_similar_column_names(newark_airport_df, citibike_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "900e3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              df1_column              df2_column  match_ratio  \\\n",
      "0  citibike_df.Trip Date  newark_airport_df.DATE          1.0   \n",
      "\n",
      "                      comment  \n",
      "0  Possible foreign key match  \n"
     ]
    }
   ],
   "source": [
    "# After reviewing the both DataFrames, the best approach to build a relationship would be use 'DATE' from 'newark_airport_df' and 'Start Time' from 'citibike_df'\n",
    "# Create a new column in citibike_df that contains only the date part of 'Start Time'\n",
    "citibike_df['Trip Date'] = pd.to_datetime(citibike_df['Start Time']).dt.date\n",
    "# Ensure that the DATE column in newark_airport_df is also in date format\n",
    "newark_airport_df['DATE'] = pd.to_datetime(newark_airport_df['DATE']).dt.date\n",
    "\n",
    "# Create a function that returns a DataFrame of columns with similar values between newark_airpor_df and citibike_df\n",
    "def possible_relationships(df1, df2, df1_name, df2_name, threshold=0.75):\n",
    "    relationships = []\n",
    "\n",
    "    for col1 in df1.columns:\n",
    "        unique_vals_1 = df1[col1].dropna().unique()\n",
    "\n",
    "        for col2 in df2.columns:\n",
    "            unique_vals_2 = df2[col2].dropna().unique()\n",
    "            # Calculate the containment ratio\n",
    "            if len(unique_vals_1) == 0:\n",
    "                continue\n",
    "\n",
    "            match_ratio = sum(np.isin(unique_vals_1, unique_vals_2)) / len(unique_vals_1)\n",
    "            if match_ratio >= threshold:\n",
    "                relationships.append({\n",
    "                    \"df1_column\": f\"{df1_name}.{col1}\",\n",
    "                    \"df2_column\": f\"{df2_name}.{col2}\",\n",
    "                    \"match_ratio\": round(match_ratio, 3),\n",
    "                    \"comment\": \"Possible foreign key match\"\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(relationships)\n",
    "\n",
    "possible_fk_df = possible_relationships(citibike_df, newark_airport_df, \"citibike_df\", \"newark_airport_df\", threshold=0.75)\n",
    "\n",
    "print(possible_fk_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "faedf062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Markdown table 'possible_fk_relationships' created successfully in 'outputs/'.\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the result from possible_relationships as a markdown table for the eda_insights.md file\n",
    "create_markdown_table(possible_fk_df, \"possible_fk_relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b90bc002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, save the cleaned DataFrames to pickle files for future use\n",
    "df_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'dataframes'))\n",
    "newark_airport_df.to_pickle(os.path.join(df_dir, \"newark_airport_df.pkl\"))\n",
    "citibike_df.to_pickle(os.path.join(df_dir, \"citibike_df.pkl\"))\n",
    "# Print a message indicating that the DataFrames have been saved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
