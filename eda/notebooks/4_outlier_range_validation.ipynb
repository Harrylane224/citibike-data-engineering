{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d44ebef",
   "metadata": {},
   "source": [
    "# **Citi Bike Data Engineering  - EDA - Outlier and Range Validation** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad88c5e",
   "metadata": {},
   "source": [
    "#### Python Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95875757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "from itertools import combinations\n",
    "from difflib import get_close_matches\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55779ab",
   "metadata": {},
   "source": [
    "#### Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53be087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically add the project root to sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from scripts.create_markdown_table import create_markdown_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ee2e5",
   "metadata": {},
   "source": [
    "#### Import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93501e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframes loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "df_dir = os.path.join(project_root, 'eda', 'dataframes')\n",
    "newark_airport_df = pd.read_pickle(os.path.join(df_dir, 'newark_airport_df.pkl'))\n",
    "citibike_df = pd.read_pickle(os.path.join(df_dir, 'citibike_df.pkl'))\n",
    "\n",
    "# Confirm that the dataframes are loaded correctly\n",
    "if newark_airport_df.empty or citibike_df.empty:\n",
    "    raise ValueError(\"One or both dataframes are empty. Please check the data loading process.\")\n",
    "else:\n",
    "    print(\"Dataframes loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9b755",
   "metadata": {},
   "source": [
    "#### Outline Business Logic Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59a41998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that sets logical bounds for newark_airport_df\n",
    "newark_airport_logical_bounds = {\n",
    "    'DATE': (dt.date(2016, 1, 1), dt.date(2016, 12, 31)),\n",
    "    'AWND':(0,100), # Assuming average wind speed in mph\n",
    "    'SNOW': (0, 10),  # Assuming snowfall is typically not more than 10 inches in a day\n",
    "    'SNWD': (0, 50),  # Assuming snow depth is typically not more than 50 inches\n",
    "    'TMAX': (0, 120),  # Assuming maximum temperature should not exceed 120°F\n",
    "    'TMIN': (-30, 80),  # Assuming minimum temperature should not be below -30°F\n",
    "    'TAVG': (-30, 120),  # Assuming average temperature should be within a reasonable range\n",
    "    'PRCP': (0, 5),  # Assuming precipitation is typically not more than 5 inches in a day\n",
    "    'WDF2': (0, 360),  # Wind direction in degrees\n",
    "    'WDF5': (0, 360),  # Wind direction in degrees\n",
    "    'WSPM': (0, 100),  # Assuming wind speed in mph\n",
    "    'WSF2': (0, 100),  # Assuming wind speed in mph\n",
    "    'WSF5': (0, 100)  # Assuming wind speed in mph\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5b8d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crete a dictionary that sets logical bounds for citibike_df\n",
    "citibike_logical_bounds = {\n",
    "    'Start Station Latitude': (40.5, 41.0),  # Approximate bounds for NYC\n",
    "    'Start Station Longitude': (-75.0, -72.0),  # Approximate bounds for NYC\n",
    "    'End Station Latitude': (40.5, 41.0),  # Approximate bounds for NYC\n",
    "    'End Station Longitude': (-75.0, -72.0),  # Approximate bounds for NYC\n",
    "    'Start Time': (pd.Timestamp('2016-01-01 00:00:00'), pd.Timestamp('2016-12-31 23:59:59')),\n",
    "    'End Time': (pd.Timestamp('2016-01-01 00:00:00'), pd.Timestamp('2016-12-31 23:59:59')),\n",
    "    'Trip Duration': (0, 86400),  # Assuming trip duration is in seconds and should not exceed 24 hours\n",
    "    'Birth Year': (1916, 2000), # Assuming participants are within a reasonable age range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bf306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for business context related to wind speeds\n",
    "wind_speed_bounds_data = [\n",
    "    {\"Description\": \"Calm to Light Breeze\", \"min_wind_speed\": 0, \"max_wind_speed\": 10, \"Notes\": \"Common daily conditions\"},\n",
    "    {\"Description\": \"Moderate to Strong Breeze\", \"min_wind_speed\": 10, \"max_wind_speed\": 40, \"Notes\": \"Typical during gusty days or storms\"},\n",
    "    {\"Description\": \"Gale to Severe Gale\", \"min_wind_speed\": 40, \"max_wind_speed\": 74, \"Notes\": \"Rare but possible in strong weather systems\"},\n",
    "    {\"Description\": \"Hurricane-force Winds\", \"min_wind_speed\": 75, \"max_wind_speed\": 100, \"Notes\": \"Extreme, often coastal or storm-driven\"}\n",
    "]\n",
    "wind_speed_df = pd.DataFrame(wind_speed_bounds_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e81e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for business context related to temperature bounds in New Jersey\n",
    "nj_temperature_bounds_data = [\n",
    "    {\"Description\": \"Frigid Winter Lows\", \"min_temp_f\": -20, \"max_temp_f\": 20, \"Notes\": \"Rare but possible in inland or northern NJ during deep winter\"},\n",
    "    {\"Description\": \"Typical Winter Range\", \"min_temp_f\": 21, \"max_temp_f\": 40, \"Notes\": \"Common from December to February\"},\n",
    "    {\"Description\": \"Mild Spring/Fall\", \"min_temp_f\": 41, \"max_temp_f\": 65, \"Notes\": \"Transitional seasons, typical March–May and October–November\"},\n",
    "    {\"Description\": \"Warm Summer Days\", \"min_temp_f\": 66, \"max_temp_f\": 85, \"Notes\": \"Typical June–August daytime highs\"},\n",
    "    {\"Description\": \"Extreme Summer Heat\", \"min_temp_f\": 86, \"max_temp_f\": 105, \"Notes\": \"Occasional heatwaves, especially in urban areas\"}\n",
    "    ]\n",
    "\n",
    "nj_temp_df = pd.DataFrame(nj_temperature_bounds_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f081ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that check the business logical bounds for a given dataframe\n",
    "def apply_business_rules(df, logical_bounds, date_columns=None):\n",
    "    \"\"\"\n",
    "    Checks if values in specified columns fall outside given logical bounds.\n",
    "    Optionally converts date columns to datetime and filters invalid values.\n",
    "    \"\"\"\n",
    "    violations = []\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert specified date columns to datetime and drop invalid entries\n",
    "    if date_columns:\n",
    "        for date_col in date_columns:\n",
    "            if date_col in df.columns:\n",
    "                df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "                df = df[df[date_col].notna()]\n",
    "\n",
    "    for col, bounds in logical_bounds.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        lower, upper = bounds\n",
    "\n",
    "        # Evaluate out-of-bounds values only where data is present\n",
    "        mask = df[col].notna() & ((df[col] < lower) | (df[col] > upper))\n",
    "        if mask.any():\n",
    "            temp_df = pd.DataFrame({\n",
    "                \"Violated Column\": col,\n",
    "                \"Lower Bound\": lower,\n",
    "                \"Upper Bound\": upper,\n",
    "                \"Identified Value\": df.loc[mask, col]\n",
    "            }).reset_index(drop=True)\n",
    "            violations.append(temp_df)\n",
    "\n",
    "    if violations:\n",
    "        return pd.concat(violations, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Violated Column\", \"Lower Bound\", \"Upper Bound\", \"Identified Value\"])\n",
    "\n",
    "# Apply business rules to both dataframes\n",
    "newark_airport_violations = apply_business_rules(newark_airport_df, newark_airport_logical_bounds)\n",
    "\n",
    "citibike_violations = apply_business_rules(citibike_df, citibike_logical_bounds, ['Start Time', 'Stop Time'])\n",
    "\n",
    "# Create a markdown table for the violations in newark_airport_df\n",
    "if not newark_airport_violations.empty:\n",
    "    newark_airport_violations_md = create_markdown_table(newark_airport_violations, \"newark_airport_logic_violations\")\n",
    "\n",
    "# Create a markdown table for the violations in citibike_df\n",
    "if not citibike_violations.empty:\n",
    "    citibike_violations_md = create_markdown_table(citibike_violations, \"citibike_logic_violations\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45a5673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to validate trip duration against start and end timestamps\n",
    "def validate_trip_duration_against_timestamps(df, duration_column, start_col, end_col, tolerance=1, min_duration=0, max_duration=86400):\n",
    "\n",
    "    # Ensure datetime types\n",
    "    df = df.copy()\n",
    "    # Check if the spcified columns is of the datetime64 type, if not convert it\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[start_col]):\n",
    "        df[start_col] = pd.to_datetime(df[start_col])\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[end_col]):\n",
    "        df[end_col] = pd.to_datetime(df[end_col])\n",
    "\n",
    "    # Calculate actual duration in seconds\n",
    "    df[\"Computed Duration\"] = (df[end_col] - df[start_col]).dt.total_seconds()\n",
    "\n",
    "    # Flag any mismatches with the added tolerance\n",
    "    mask = (\n",
    "        (df[duration_column] != df[\"Computed Duration\"]).abs() > tolerance |\n",
    "        (df[\"Computed Duration\"] < min_duration) |\n",
    "        (df[\"Computed Duration\"] > max_duration)\n",
    "    )\n",
    "\n",
    "    violations = df.loc[mask, [start_col, end_col, duration_column, \"Computed Duration\"]].copy()\n",
    "    violations[\"Violated Column\"] = \"Trip Duration (derived)\"\n",
    "    violations[\"Expected Range\"] = f\"{min_duration}-{max_duration} seconds\"\n",
    "\n",
    "    return violations.reset_index(drop=True)\n",
    "\n",
    "citibike_duration_violations = validate_trip_duration_against_timestamps(citibike_df,'Trip Duration','Start Time','Stop Time')\n",
    "\n",
    "# Create a markdown table for the trip duration violations\n",
    "if not citibike_duration_violations.empty:\n",
    "    citibike_duration_violations_md = create_markdown_table(citibike_duration_violations, \"citibike_trip_duration_violations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ca367",
   "metadata": {},
   "source": [
    "#### Detect Categorical and Quantitative Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate quantitative outlier distributions, using the IQR method and Z-scores\n",
    "def quantitative_outlier_distribution(df, iqr_factor=1.5, z_thresh=3):\n",
    "    \"\"\"\n",
    "    For each numeric column in the dataframe:\n",
    "    - Calculate IQR bounds and outlier count\n",
    "    - Count of Q1 and Q2 quartile values\n",
    "    - Mean and Std\n",
    "    - Z-score thresholds and counts\n",
    "    - Returns a summary dataframe with each metric in a separate column\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        s = df[col].dropna()\n",
    "        if s.empty:\n",
    "            continue\n",
    "\n",
    "        # IQR stats\n",
    "        Q1 = s.quantile(0.25)\n",
    "        Q2 = s.quantile(0.50)\n",
    "        Q3 = s.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        iqr_lower = Q1 - iqr_factor * IQR\n",
    "        iqr_upper = Q3 + iqr_factor * IQR\n",
    "\n",
    "        # IQR segment counts\n",
    "        q1_to_q2 = s[(s >= Q1) & (s < Q2)].count()\n",
    "        q2_to_q3 = s[(s >= Q2) & (s <= Q3)].count()\n",
    "        iqr_outliers = s[(s < iqr_lower) | (s > iqr_upper)].count()\n",
    "\n",
    "        # Z-score bounds\n",
    "        mean = s.mean()\n",
    "        std = s.std()\n",
    "        z_lower = mean - z_thresh * std\n",
    "        z_upper = mean + z_thresh * std\n",
    "\n",
    "        z_inlier_count = s[(s >= z_lower) & (s <= z_upper)].count()\n",
    "        z_outlier_count = s[(s < z_lower) | (s > z_upper)].count()\n",
    "\n",
    "        summary.append({\n",
    "            \"Column\": col,\n",
    "            \"IQR Range\": f\"[{round(Q1, 2)}-{round(Q3, 2)}]\",\n",
    "            \"Q1-Q2\": q1_to_q2,\n",
    "            \"Q2-Q3\": q2_to_q3,\n",
    "            \"IQR Outliers\": iqr_outliers,\n",
    "            \"Mean\": round(mean, 2),\n",
    "            \"Std\": round(std, 2),\n",
    "            \"Z-Score Range\": f\"[{round(z_lower, 2)}-{round(z_upper, 2)}]\",\n",
    "            \"Z-Score Inliers\": z_inlier_count,\n",
    "            \"Z-Score Outliers\": z_outlier_count\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# Apply the quantitative outlier bins function to both dataframes\n",
    "newark_airport_quantitative_outliers = quantitative_outlier_distribution(newark_airport_df)\n",
    "citibike_quantitative_outliers = quantitative_outlier_distribution(citibike_df)\n",
    "\n",
    "# Create markdown tables for the quantitative outlier distributions\n",
    "if not newark_airport_quantitative_outliers.empty:\n",
    "    newark_airport_quantitative_outliers_md = create_markdown_table(newark_airport_quantitative_outliers, \"newark_airport_quantitative_outliers\")\n",
    "if not citibike_quantitative_outliers.empty:\n",
    "    citibike_quantitative_outliers_md = create_markdown_table(citibike_quantitative_outliers, \"citibike_quantitative_outliers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60719617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unique Categories",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Top 3 Categories",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Dominant Category (%)",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Rare Categories (< 1%)",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "7e20c07d-fff5-4085-abf2-a5dbd36c6715",
       "rows": [
        [
         "0",
         "Start Time",
         "244407",
         "2016-09-28 08:24:23 (0.0%), 2016-08-16 08:35:32 (0.0%), 2016-11-02 08:42:16 (0.0%)",
         "0.0",
         "244407"
        ],
        [
         "1",
         "Stop Time",
         "244137",
         "2016-04-17 17:33:34 (0.0%), 2016-10-01 15:30:25 (0.0%), 2016-08-16 08:52:28 (0.0%)",
         "0.0",
         "244137"
        ],
        [
         "2",
         "Start Station Name",
         "51",
         "Grove St PATH (11.6%), Exchange Place (7.7%), Sip Ave (6.9%)",
         "11.6",
         "22"
        ],
        [
         "3",
         "End Station Name",
         "102",
         "Grove St PATH (15.5%), Exchange Place (9.0%), Sip Ave (6.4%)",
         "15.5",
         "76"
        ],
        [
         "4",
         "User Type",
         "2",
         "Subscriber (93.7%), Customer (6.3%)",
         "93.7",
         "0"
        ],
        [
         "5",
         "Trip Date",
         "362",
         "2016-09-13 (0.6%), 2016-09-22 (0.5%), 2016-09-15 (0.5%)",
         "0.6",
         "362"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 6
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Unique Categories</th>\n",
       "      <th>Top 3 Categories</th>\n",
       "      <th>Dominant Category (%)</th>\n",
       "      <th>Rare Categories (&lt; 1%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Start Time</td>\n",
       "      <td>244407</td>\n",
       "      <td>2016-09-28 08:24:23 (0.0%), 2016-08-16 08:35:3...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>244407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stop Time</td>\n",
       "      <td>244137</td>\n",
       "      <td>2016-04-17 17:33:34 (0.0%), 2016-10-01 15:30:2...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>244137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Start Station Name</td>\n",
       "      <td>51</td>\n",
       "      <td>Grove St PATH (11.6%), Exchange Place (7.7%), ...</td>\n",
       "      <td>11.6</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>End Station Name</td>\n",
       "      <td>102</td>\n",
       "      <td>Grove St PATH (15.5%), Exchange Place (9.0%), ...</td>\n",
       "      <td>15.5</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User Type</td>\n",
       "      <td>2</td>\n",
       "      <td>Subscriber (93.7%), Customer (6.3%)</td>\n",
       "      <td>93.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Trip Date</td>\n",
       "      <td>362</td>\n",
       "      <td>2016-09-13 (0.6%), 2016-09-22 (0.5%), 2016-09-...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Column  Unique Categories  \\\n",
       "0          Start Time             244407   \n",
       "1           Stop Time             244137   \n",
       "2  Start Station Name                 51   \n",
       "3    End Station Name                102   \n",
       "4           User Type                  2   \n",
       "5           Trip Date                362   \n",
       "\n",
       "                                    Top 3 Categories  Dominant Category (%)  \\\n",
       "0  2016-09-28 08:24:23 (0.0%), 2016-08-16 08:35:3...                    0.0   \n",
       "1  2016-04-17 17:33:34 (0.0%), 2016-10-01 15:30:2...                    0.0   \n",
       "2  Grove St PATH (11.6%), Exchange Place (7.7%), ...                   11.6   \n",
       "3  Grove St PATH (15.5%), Exchange Place (9.0%), ...                   15.5   \n",
       "4                Subscriber (93.7%), Customer (6.3%)                   93.7   \n",
       "5  2016-09-13 (0.6%), 2016-09-22 (0.5%), 2016-09-...                    0.6   \n",
       "\n",
       "   Rare Categories (< 1%)  \n",
       "0                  244407  \n",
       "1                  244137  \n",
       "2                      22  \n",
       "3                      76  \n",
       "4                       0  \n",
       "5                     362  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function to calculate categorical outlier distributions, using frequency counts\n",
    "def categorical_outlier_distribution(df, top_n=3, rare_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Summarizes each categorical column with:\n",
    "    - Unique count\n",
    "    - Top frequent categories\n",
    "    - Rare category count\n",
    "    - Dominance of top class\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        s = df[col].dropna()\n",
    "        total = len(s)\n",
    "        if total == 0:\n",
    "            continue\n",
    "\n",
    "        value_counts = s.value_counts(normalize=True)\n",
    "        absolute_counts = s.value_counts()\n",
    "\n",
    "        # Top categories\n",
    "        top_categories = \", \".join([f\"{idx} ({round(pct*100, 1)}%)\"\n",
    "                                    for idx, pct in value_counts.head(top_n).items()])\n",
    "\n",
    "        # Rarity\n",
    "        rare_count = (value_counts < rare_threshold).sum()\n",
    "\n",
    "        summary.append({\n",
    "            \"Column\": col,\n",
    "            \"Unique Categories\": s.nunique(),\n",
    "            f\"Top {top_n} Categories\": top_categories,\n",
    "            \"Dominant Category (%)\": round(value_counts.iloc[0]*100, 1),\n",
    "            f\"Rare Categories (< {int(rare_threshold*100)}%)\": rare_count\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# Apply the categorical outlier bins function to both dataframes\n",
    "newark_airport_categorical_outliers = categorical_outlier_distribution(newark_airport_df)\n",
    "citibike_categorical_outliers = categorical_outlier_distribution(citibike_df)\n",
    "\n",
    "citibike_categorical_outliers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
